# MEDS-pipeline
MEDS pipeline for both MIMIC and AHS datasets

**Note**: This pipeline only supports MEDS-CORE format. MEDS-PLUS has been removed.

## Quick Start Examples

```bash
# Test with MIMIC dataset (100 patients)
python3 -m meds_pipeline.cli run --source mimic --components ecgs,admissions,eds,diagnosis,procedures,medicines,demographics --cfg mimic.yaml --max-patients 100

# AHS medicines component (100 patients)
python3 -m meds_pipeline.cli run --source ahs --components ecgs,admissions,eds,diagnosis,procedures,medicines,demographics --cfg ahs.yaml --max-patients 100

# MIMIC with multiple components (100 patients, CORE format - default)
PYTHONPATH=src python3 -m meds_pipeline.cli run --source mimic --components admissions,eds --cfg src/meds_pipeline/configs/mimic.yaml --max-patients 100 --progress

# AHS with multiple components (100 patients, CORE format - default)
PYTHONPATH=src python3 -m meds_pipeline.cli run --source ahs --components admissions,eds --cfg src/meds_pipeline/configs/ahs.yaml --max-patients 100 --progress
```

## Progress Display and Patient Limiting Features

### New Features Overview

This MEDS pipeline includes two important features:

#### 1. Detailed Progress Display
- ğŸ“– Data loading phase: Shows file reading progress and row counts
- ğŸ“Š Component processing: Shows rows and patient counts generated by each component
- ğŸ”— Data merging: Shows final merge results
- â±ï¸ Processing time: Shows overall processing time

#### 2. Patient Count Limiting
- ğŸ§ª Test mode: Limit processing to a specific number of patients
- ğŸš€ Quick validation: Verify pipeline functionality on small datasets
- ğŸ“Š Resource control: Avoid processing oversized datasets during testing

## Usage

### Basic Command Format
```bash
python3 -m meds_pipeline.cli run \
  --source <mimic|ahs> \
  --components <component1,component2> \
  [--max-patients N] \
  [--progress|--no-progress]
```

### Example Usage

#### 1. Test Mode (Recommended for Getting Started)
```bash
# Process only 100 patients for quick testing (CORE format is default)
python3 -m meds_pipeline.cli run \
  --source mimic \
  --components medicines \
  --max-patients 100 \
  --progress
```

#### 2. Medium-Scale Testing
```bash
# Process 1000 patients
python3 -m meds_pipeline.cli run \
  --source mimic \
  --components admissions,medicines \
  --max-patients 1000
```

#### 3. Full Run (Production Mode)
```bash
# Process all patients
python3 -m meds_pipeline.cli run \
  --source mimic \
  --components admissions,medicines \
  --progress
```

#### 4. Silent Mode (No Progress Display)
```bash
# No detailed progress information
python3 -m meds_pipeline.cli run \
  --source mimic \
  --components medicines \
  --max-patients 500 \
  --no-progress
```

## New Output Example

With progress display enabled, you'll see output like this:

```
ğŸš€ Processing 1 components...
ğŸ“Š Patient limit: 100
============================================================

ğŸ“‹ Component 1/1: medicines
ğŸ“– Loading medication data...
   â””â”€ Loaded 1,234,567 rows
   â””â”€ Limited to 100/45,678 patients, 2,345 rows
   âœ… Generated 2,345 rows for 100 patients

ğŸ”— Combining all components...
   âœ… Final result: 2,345 rows for 100 patients

Data processing completed in 15.23 seconds
Generated 2,345 rows for 100 unique patients
```

Note on output chunking: when the pipeline's output contains more than the default chunk size (100,000 rows), results are split into multiple files named `<source>_meds_core_part_{NNN}.parquet` and saved under `base.output_dir/<source>/`; otherwise a single file `<source>_meds_core.parquet` is written.

## Parameter Reference

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `--max-patients` | int | None | Limit the number of patients to process for testing |
| `--progress` | flag | True | Show detailed progress information |
| `--no-progress` | flag | False | Hide progress information |
| `--source` | choice | Required | Data source: mimic or ahs |
| `--components` | string | Required | Comma-separated list of components |

## Recommended Workflow

### First Time Usage
1. **Small-scale test**: Use `--max-patients 10` for quick verification
2. **Medium test**: Use `--max-patients 100` to check result quality
3. **Large-scale test**: Use `--max-patients 1000` to verify performance
4. **Production run**: Remove `--max-patients` to process all data

### Debugging Issues
1. Use `--max-patients 10 --progress` for the most detailed debugging information
2. Check if output row counts and patient counts meet expectations
3. If issues arise, start with small datasets and gradually increase scale

## Performance Optimization

- Patient limiting takes effect during the data loading phase, avoiding unnecessary data processing
- Progress display helps you understand which steps are most time-consuming
- You can optimize data processing workflows based on progress information

## Patient Split Functionality

### Overview

The pipeline includes a separate script to split patients into development (60%) and holdout (40%) sets. This split is performed randomly using a fixed seed (42) to ensure reproducibility.

### Usage

The `split_patients.py` script is located in the `src/` directory and can be run independently:

```bash
# Split MIMIC patients
PYTHONPATH=src python src/split_patients.py --source mimic --cfg mimic.yaml

# Split AHS patients
PYTHONPATH=src python src/split_patients.py --source ahs --cfg ahs.yaml

# Custom output path
PYTHONPATH=src python src/split_patients.py --source mimic --cfg mimic.yaml --output custom_split.parquet

# Custom development ratio (default is 0.6)
PYTHONPATH=src python src/split_patients.py --source mimic --cfg mimic.yaml --dev-ratio 0.7
```

### Output Format

The script generates a **Parquet file** (default: `src/meds_pipeline/configs/{source}_split.parquet`) with the following structure:

**DataFrame Structure:**
- **Index**: `patient_id` (string) - unique patient identifiers
- **Column**: `split` (categorical string) - values: `"development"` or `"holdout"`

**Metadata File:**
A companion JSON file (`{source}_split.json`) is also generated containing:
- `seed`: Random seed used (42)
- `dev_ratio`: Development set ratio (0.6)
- `holdout_ratio`: Holdout set ratio (0.4)
- `total_patients`: Total number of patients
- `dev_patients`: Number of patients in development set
- `holdout_patients`: Number of patients in holdout set
- `schema`: DataFrame schema information

### Loading the Split

```python
import pandas as pd
from meds_pipeline.utils.patient_split import load_split

# Load the split DataFrame
df_split = load_split("src/meds_pipeline/configs/mimic_split.parquet")

# Or load directly with pandas
df_split = pd.read_parquet("src/meds_pipeline/configs/mimic_split.parquet")

# Access patient IDs by split type
dev_patients = df_split[df_split['split'] == 'development'].index.tolist()
holdout_patients = df_split[df_split['split'] == 'holdout'].index.tolist()

# Check if a specific patient is in development set
patient_id = "12345"
is_dev = df_split.loc[patient_id, 'split'] == 'development'
```

### Legacy YAML Format

For backward compatibility, you can still generate YAML format using `--format yaml`:

```bash
PYTHONPATH=src python src/split_patients.py --source mimic --cfg mimic.yaml --format yaml
```

### Notes

- The split uses the seed from `base.yaml` (default: 42) for reproducibility
- Patient IDs are read from the admissions file specified in the config
- The script handles both MIMIC (CSV) and AHS (SAS/pickle) file formats automatically
- **Parquet format** is more efficient for large datasets and enables fast lookups by patient_id
- The `split` column uses categorical dtype for memory efficiency
- Metadata is saved separately as JSON for easy access to split statistics

## Troubleshooting

### Common Issues
1. **Out of memory**: Use smaller `--max-patients` values
2. **Processing too slow**: Check progress output to find bottleneck steps
3. **Patient count doesn't match expectations**: Check the `subject_id` column in source data

### Debugging Steps
1. Start testing with `--max-patients 10`
2. Check output statistics for each component
3. Gradually increase patient count until you find the appropriate scale

TODO:

write build_schema
write write_df
Write Test

## Project Structure

```
MEDS-pipeline/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ split_patients.py      # Independent script for patient splitting
â”‚   â”œâ”€â”€ args.py
â”‚   â””â”€â”€ meds_pipeline/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ cli.py
â”‚       â”œâ”€â”€ configs/
â”‚       â”‚   â”œâ”€â”€ ahs.yaml
â”‚       â”‚   â”œâ”€â”€ base.yaml
â”‚       â”‚   â”œâ”€â”€ mimic.yaml
â”‚       â”‚   â”œâ”€â”€ ahs_split.parquet      # Generated by split_patients.py
â”‚       â”‚   â”œâ”€â”€ ahs_split.json         # Metadata for ahs_split.parquet
â”‚       â”‚   â”œâ”€â”€ mimic_split.parquet    # Generated by split_patients.py
â”‚       â”‚   â””â”€â”€ mimic_split.json       # Metadata for mimic_split.parquet
â”‚       â”œâ”€â”€ utils/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ patient_split.py   # Patient split utilities
â”‚       â””â”€â”€ etl/
â”‚           â”œâ”€â”€ base.py
â”‚           â”œâ”€â”€ registry.py
â”‚           â”œâ”€â”€ ahs/
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â””â”€â”€ admissions.py
â”‚           â”‚   â””â”€â”€ ...
â”‚           â””â”€â”€ mimic/
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â””â”€â”€ admissions.py
â”‚           â”‚   â””â”€â”€ ...
â”‚           â””â”€â”€ orchestrators/
â”‚           â”‚   â”œâ”€â”€ __init__.py
â”‚           â”‚   â””â”€â”€ ahs_source.py
â”‚           â”‚   â””â”€â”€ mimic_source.py
â””â”€â”€ tests/
```
